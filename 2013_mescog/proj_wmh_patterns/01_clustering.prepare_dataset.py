# -*- coding: utf-8 -*-
"""
Created on Wed Dec 18 13:45:32 2013

@author: md238665

Create a mask from all the data and then split dataset for clustering.
We separate French subjects (id < 2000) from German ones (id >= 2000).
It should work even if the subject ID file is not sorted.
The mask is any voxel which has a WMH on a subject (by construction,
it is a subset of MNI mask).
Optionnaly center data.

INPUT:
 - INPUT_DATASET: the numpy array generated by 00_build_dataset
 - INPUT_SUBJECTS: the subjects index generated by 00_build_dataset
 - INPUT_MASK: the MNI mask

OUTPUT:
 - OUTPUT_DATASETS: train and test datasets
 - OUTPUT_STD_DATASETS: standardized train and test datasets
 - OUTPUT_SCALER: sklearn StandandardScaler object used to scale datasets
 - OUTPUT_MASK: mask of selected voxels (subset of INPUT_MASK)

TODO:
 - mask computation should be in 00_build_dataset.
   By the way the voxel extaction using MNI mask should be done there too.
 - erode and dilate mask?
 - output in the dataset directory?

"""

import os
import pickle

import numpy as np

import sklearn
import sklearn.preprocessing

import nibabel

##################
# Input & output #
##################

INPUT_BASE_DIR = "/neurospin/"
INPUT_DIR = os.path.join(INPUT_BASE_DIR,
                         "mescog", "datasets")
INPUT_DATASET = os.path.join(INPUT_DIR,
                             "CAD-WMH-MNI.npy")
INPUT_SUBJECTS = os.path.join(INPUT_DIR,
                              "CAD-WMH-MNI-subjects.txt")

INPUT_RESOURCES_DIR = os.path.join(INPUT_BASE_DIR,
                                   "mescog", "neuroimaging", "ressources")
INPUT_MASK = os.path.join(INPUT_RESOURCES_DIR,
                          "MNI152_T1_1mm_brain_mask.nii.gz")

OUTPUT_BASE_DIR = "/neurospin/"
OUTPUT_DIR = os.path.join(OUTPUT_BASE_DIR,
                          "mescog", "proj_wmh_patterns")
if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)

OUTPUT_FEATURES_MASK = os.path.join(OUTPUT_DIR, "features_mask.nii")
OUTPUT_MASK = os.path.join(OUTPUT_DIR, "wmh_mask.nii")
OUTPUT_SUBJECTS = [os.path.join(OUTPUT_DIR, f) for f in ["french-subjects.txt",
                                                         "germans-subjects.txt"]]
OUTPUT_DATASETS = [os.path.join(OUTPUT_DIR, f) for f in ["french.npy",
                                                         "germans.npy"]]
OUTPUT_STD_DATASETS = [os.path.join(OUTPUT_DIR, f) for f in ["french.center.npy",
                                                             "germans.center.npy"]]
OUTPUT_SCALER = os.path.join(OUTPUT_DIR, "scaler.pkl")

##############
# Parameters #
##############

MAX_TRAIN_ID = 2000
IM_SHAPE = (182, 218, 182)

#################
# Actual script #
#################

# Read subjects
with open(INPUT_SUBJECTS) as f:
    subjects_id = np.asarray([int(line.rstrip()) for line in f.readlines()])
n_subjects = len(subjects_id)
print "Found", n_subjects, "subjects"

# Read masked images
X = np.load(INPUT_DATASET)
ORIG_SHAPE = X.shape
print "Loaded images dataset {s}".format(s=ORIG_SHAPE)

# Open MNI mask
babel_mni_mask = nibabel.load(INPUT_MASK)
mni_mask = babel_mni_mask.get_data() != 0
n_voxels_in_mask = np.count_nonzero(mni_mask)
print "MNI mask: {n} voxels".format(n=n_voxels_in_mask)

# Extract features (it is a subset of MNI mask)
features_mask = np.any(X != 0, axis=0)
mask_index = np.where(features_mask)[0]
n_features = mask_index.shape[0]
print "Found {n} features".format(n=n_features)

# Create mask & save it
mask = np.zeros(mni_mask.shape, dtype=bool)
mask[mni_mask] = features_mask
mask_babel = nibabel.Nifti1Image(mask.astype(np.uint8),
                                 babel_mni_mask.get_affine())
nibabel.save(mask_babel, OUTPUT_MASK)

# Get indices of French and German subjects.
# We use np.where so it works even with non-ordered
# subjects list (and the order of each subset will be the same).
# There might be faster ways to extract test indices with sets and so on
# but it is not worth.
inf = subjects_id < MAX_TRAIN_ID
train_index = np.where(inf)[0]
print "Found", len(train_index), "training subjects"
test_index = np.where(~inf)[0]
print "Found", len(test_index), "testing subjects"

# Extract & save subsets
train = X[train_index][:, mask_index]
test = X[test_index][:, mask_index]
del X

np.savetxt(OUTPUT_SUBJECTS[0], subjects_id[train_index], '%s')
np.savetxt(OUTPUT_SUBJECTS[1], subjects_id[test_index], '%s')
np.save(OUTPUT_DATASETS[0], train)
np.save(OUTPUT_DATASETS[1], test)
print "Train and test data saved"

# Standardize sets and save
scaler = sklearn.preprocessing.StandardScaler(with_std=False)
scaler.fit(train)
train_std = scaler.transform(train)
test_std = scaler.transform(test)

np.save(OUTPUT_STD_DATASETS[0], train_std)
np.save(OUTPUT_STD_DATASETS[1], test_std)
with open(OUTPUT_SCALER, "wb") as f:
    pickle.dump(scaler, f)
print "Standardized train and test data saved"
