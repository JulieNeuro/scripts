# -*- coding: utf-8 -*-
"""
Created on Wed Dec 18 13:45:32 2013

@author: md238665

Create a mask from all the data and then split dataset for clustering.
We separate French subjects (id < 2000) from German ones (id >= 2000).
It should work even if the subject ID file is not sorted.
The mask is any voxel which has a WMH on a subject (by construction,
it is a subset of MNI mask).
Optionnaly center data.

INPUT:
 - INPUT_DATASET: the numpy array generated by 00_build_dataset
 - INPUT_SUBJECTS: the subjects index generated by 00_build_dataset
 - INPUT_MASK: the MNI mask

OUTPUT:
 - OUTPUT_DATASETS: train and test datasets
 - OUTPUT_STD_DATASETS: standardized train and test datasets
 - OUTPUT_SCALER: means used to scale datasets

TODO:
 - erode and dilate mask?
 - output in the dataset directory?

"""

import os

import numpy as np

import sklearn
import sklearn.preprocessing

##################
# Input & output #
##################

BASE_DIR = os.path.join("/neurospin",
                        "mescog", "proj_wmh_patterns")
INPUT_DIR = BASE_DIR
INPUT_DATASET = os.path.join(INPUT_DIR,
                             "CAD-WMH-MNI.npy")
INPUT_SUBJECTS = os.path.join(INPUT_DIR,
                              "CAD-WMH-MNI-subjects.txt")

OUTPUT_DIR = BASE_DIR
if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)

OUTPUT_SUBJECTS = [os.path.join(OUTPUT_DIR, f) for f in ["french-subjects.txt",
                                                         "germans-subjects.txt"]]
OUTPUT_DATASETS = [os.path.join(OUTPUT_DIR, f) for f in ["french.npy",
                                                         "germans.npy"]]
OUTPUT_STD_DATASETS = [os.path.join(OUTPUT_DIR, f) for f in ["french.center.npy",
                                                             "germans.center.npy"]]
OUTPUT_SCALER = os.path.join(OUTPUT_DIR, "scaler.npy")

##############
# Parameters #
##############

MAX_TRAIN_ID = 2000
IM_SHAPE = (182, 218, 182)

#################
# Actual script #
#################

# Read subjects
with open(INPUT_SUBJECTS) as f:
    subjects_id = np.asarray([int(line.rstrip()) for line in f.readlines()])
n_subjects = len(subjects_id)
print "Found", n_subjects, "subjects"

# Read masked images
X = np.load(INPUT_DATASET)
ORIG_SHAPE = X.shape
print "Loaded images dataset {s}".format(s=ORIG_SHAPE)

# Get indices of French and German subjects.
# We use np.where so it works even with non-ordered
# subjects list (and the order of each subset will be the same).
# There might be faster ways to extract test indices with sets and so on
# but it is not worth.
inf = subjects_id < MAX_TRAIN_ID
french_index = np.where(inf)[0]
print "Found", len(french_index), "french subjects"
germans_index = np.where(~inf)[0]
print "Found", len(germans_index), "german subjects"

# Extract & save subsets
french = X[french_index]
germans = X[germans_index]
del X

np.savetxt(OUTPUT_SUBJECTS[0], subjects_id[french_index], '%s')
np.savetxt(OUTPUT_SUBJECTS[1], subjects_id[germans_index], '%s')
np.save(OUTPUT_DATASETS[0], french)
np.save(OUTPUT_DATASETS[1], germans)
print "Data saved"

# Standardize sets and save
scaler = sklearn.preprocessing.StandardScaler(with_std=False)
scaler.fit(french)
french_centered = scaler.transform(french)
germans_centered = scaler.transform(germans)

np.save(OUTPUT_STD_DATASETS[0], french_centered)
np.save(OUTPUT_STD_DATASETS[1], germans_centered)
np.save(OUTPUT_SCALER, scaler.mean_)
print "Centered data saved"
