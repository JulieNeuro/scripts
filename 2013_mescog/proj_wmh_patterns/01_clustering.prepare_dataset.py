# -*- coding: utf-8 -*-
"""
Created on Wed Dec 18 13:45:32 2013

@author: md238665

Create a mask from all the data and then split dataset for clustering.
We separate French subjects (id < 2000) from German ones (id >= 2000).
It should work even if the subject ID file is not sorted.
The mask is any voxel which has a WMH on a subject.

INPUT:
 - INPUT_DATASET: the numpy array generated by 00_build_dataset
 - INPUT_SUBJECTS: the subjects index generated by 00_build_dataset

OUTPUT:
 - OUTPUT_DATASETS: train and test datasets
 - OUTPUT_STD_DATASETS: standardized train and test datasets
 - OUTPUT_SCALER: sklearn StandandardScaler object used to scale datasets

TODO:
 - mask should computation should be in 00_build_dataset (where we can save
   an image)
 - add dilation in mask?
 - change input and output base directories
 - output in the dataset directory?
 - output file names (french, german)?

"""

import os
import pickle

import numpy as np

import sklearn
import sklearn.preprocessing

##################
# Input & output #
##################

INPUT_BASE_DIR = "/volatile/"
INPUT_DIR = os.path.join(INPUT_BASE_DIR,
                         "mescog", "datasets")
INPUT_DATASET = os.path.join(INPUT_DIR,
                             "CAD-WMH-MNI.npy")
INPUT_SUBJECTS = os.path.join(INPUT_DIR,
                              "CAD-WMH-MNI-subjects.txt")

OUTPUT_BASE_DIR = "/volatile/"
OUTPUT_DIR = os.path.join(OUTPUT_BASE_DIR,
                          "mescog", "results")
if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)

OUTPUT_SUBJECTS = [os.path.join(OUTPUT_DIR, f) for f in ["train_subjects.txt",
                                                         "test_subjects.txt"]]
OUTPUT_DATASETS = [os.path.join(OUTPUT_DIR, f) for f in ["train.npy",
                                                         "test.npy"]]
OUTPUT_STD_DATASETS = [os.path.join(OUTPUT_DIR, f) for f in ["train.std.npy",
                                                             "test.std.npy"]]
OUTPUT_SCALER = os.path.join(OUTPUT_DIR, "scaler.pkl")

##############
# Parameters #
##############

MAX_TRAIN_ID = 2000
IM_SHAPE = (91, 109, 91)

#################
# Actual script #
#################

# Read data
with open(INPUT_SUBJECTS) as f:
    subjects_id = np.asarray([int(line.rstrip()) for line in f.readlines()])
n_subjects = len(subjects_id)
print "Found", n_subjects, "subjects"

X = np.load(INPUT_DATASET)
ORIG_SHAPE = X.shape
print "Loaded images dataset {s}".format(s=ORIG_SHAPE)

# Extract mask
NEW_SHAPE = (n_subjects, ) + IM_SHAPE
X.shape = NEW_SHAPE  # Reshape without copy (or raise exception)
mask = np.any(X, axis=0)
n_features = np.count_nonzero(mask)
print "Found", n_features, "features"
mask_lin = mask.ravel()
mask_index = np.where(mask_lin)[0]
X.shape = ORIG_SHAPE  # Restore shape

# Get indices of French and German subjects.
# We use np.where so it works even with non-ordered
# subjects list (and the order of each subset will be the same).
# There migh be faster ways to extract test indices with sets and so on
# but it is not worth.
inf = subjects_id < MAX_TRAIN_ID
train_index = np.where(inf)[0]
print "Found", len(train_index), "training subjects"
test_index = np.where(~inf)[0]
print "Found", len(test_index), "testing subjects"

# Extract & save subsets
train = X[train_index][:, mask_index]
test = X[test_index][:, mask_index]
del X

np.savetxt(OUTPUT_SUBJECTS[0], subjects_id[train_index], '%s')
np.savetxt(OUTPUT_SUBJECTS[1], subjects_id[test_index], '%s')
np.save(OUTPUT_DATASETS[0], train)
np.save(OUTPUT_DATASETS[1], test)
print "Train and test data saved"

# Standardize sets and save
scaler = sklearn.preprocessing.StandardScaler()
scaler.fit(train)
train_std = scaler.transform(train)
test_std = scaler.transform(test)

np.save(OUTPUT_STD_DATASETS[0], train_std)
np.save(OUTPUT_STD_DATASETS[1], test_std)
with open(OUTPUT_SCALER, "wb") as f:
    pickle.dump(scaler, f)
print "Standardized train and test data saved"
